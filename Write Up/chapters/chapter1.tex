%!TEX root = ../dissertation.tex

\chapter{Background}

\newthought{The randomized control trial} introduced in the Introduction seeks to prove causation, allowing for concrete recommendations about the best course of treatment.  The randomization in these trials is what allows the conclusion of causation to be drawn, rather than just correlation.  Randomization intends to eliminate the potential bias caused by factors other than the treatment of interest.  By randomizing subjects into control and treatment groups, neither group should be more likely to respond to treatment.  Because of this, any difference that is seen should be purely due to the treatment and not exogenous factors.  

However, not only are randomized control trials costly and slow, they can be unethical.  For example, forcing subjects to smoke cigarettes in order to determine if smoking causes lung cancer is not morally appropriate.  As such, the motivation to use observational data to answer such causation questions is strong.  The field of causal inference seeks to accomplish exactly this for both simple observational data and for data from complex dynamically assigned randomized trial.  The field of causal inference began with Neyman in 1990 and Fisher in 1925, who performed agricultural studies to test the efficacy of certain fertilizers on crop yields.\cite{edwards2005ra, fisher1935design, splawa1990application}  Beginning in the 1970s, Rubin advanced and formalized the field, providing more structured frameworks for understanding and computing causal effects.\cite{rosenbaum1984reducing, rubin1974estimating, rubin1978bayesian, rubin1984william} The methods discussed in this thesis and introduced formally in Chapter \ref{Methods}, the g-formula and doubly robust estimators, were introduced by Robins in 1986 and 2005 respectively.\cite{bang2005doubly, robins1986new}

\section{Causal Effects} 
To translate this idea into statistical terms, some notation must be introduced.  The random variable $A$ represents the treatment status, where a value of $1$ indicates treated and a value of $0$ indicates untreated.  A fixed $A$, which has constant treatment over time, is written as $A_i$ for an individual $i \in \{0,1,2,\dots n\}$ with $n$ the total number of individuals.\footnote{Non-fixed $A$ representations are common and discussed in greater detail in Section \ref{Time-varying}.} The random variable $Y$ is the outcome variable, often with a value of $0$ indicating survival and a value of $1$ indicating death.  These interpretations of $A$ and $Y$ correspond to the above understanding of causation studies, but for various causal inference studies, the form of $Y$ and $A$ in particular can change depending on the question of interest.  For example, $Y$ can be a continuous variable, such as the weight difference of an individual in a weight loss trial or the change in HDL levels in a cholesterol study. 

To study the causal effect of $A$ on $Y$, the desired value is the difference in $Y$ under the varying conditions of $A$.  Notationally, this is the difference between $Y^{\, a=1}$ \footnote{Note that lowercase letters signify possible values of the random variable, in comparison to uppercase letters which represent actual observed values}, the outcome that would be observed under treatment, and $Y^{\, a=0}$, the outcome that would be observed under no treatment.  This is in comparison to the observed outcome of $Y$ or $Y^A$.  

A causal effect can be seen on an individual level if the difference $\; Y_i^{\, a=1} \neq Y_i^{\, a=0}$ is significant.  By considering how each individual's responses to varying treatments differ, causation (or lack thereof) can easily be determined using paired differences of the form 
\begin{align} \; Y_i^{\, a=1} - Y_i^{\, a=0}\end{align} 
These differences would be tested against the null hypothesis of zero difference in outcome for varying treatments.  

However, certain difficulties arise using this method.  In many studies, it is impossible to have scenarios of both treatment and no treatment for the same individual, particularly if a potential outcome is death.  Typically, individuals either have $\; Y_i^{\, a=1}$ or  $Y_i^{\, a=0}$, but not both, making it impossible to calculate the paired differences.  Therefore, a controlled double blinded experiment is often performed, where each individual is randomly assigned treatment or placebo.  In these studies, the statistic of interest is the average causal effect in the population, 
\begin{align}  \mathbb{E}[Y^{\; a=1}] - \mathbb{E}[Y^{\; a=0}] \end{align} 
Mathematically, this is equivalent to 
\begin{align}  \mathbb{E}[Y^{\; a=1} - Y^{\; a=0}] \end{align}  
because the average of differences is equal to the difference of averages.\cite{hernan_robins_2016} Note, that this is not the same as calculating the mean of paired differences as if each individual had received both treatments at different times to calculate individual causal effects.  Rather, the difference in the means of the placebo and treatment groups is being calculated to estimate average causal effect across the population.  

\subsection{Time Varying Data} \label{Time-varying}
Not all treatment regimens consist of constant treatment over a set period of time.  Therefore, a more complicated time-varying treatment can be considered.  This would be written for a single individual as $\overline{A}_k = \{A_0, A_1, \cdots, A_k \}$, with time point $k \in \{ 0, 1, \dots K\}$, given $K$ as the maximum time value.  The overline on $\overline{A}_k$ indicates the history of values up to and including time point $k$, and the notation $\overline{A}$ represents the full history.  As an example of this, a patient with continuous treatment throughout the whole study would have data $\overline{A} = \{A_0 = 1, A_1 = 1, \dots, A_K = 1 \}  = \{1,1,\dots 1 \}$, which can also be written as $\overline{A} = \overline{1}$.  In this scenario, the average causal effect is instead defined as 
\begin{align} \mathbb{E}\bigg[Y^{\bar{a} = \bar{1}}\bigg] -  \mathbb{E}\bigg[Y^{\bar{a} = \bar{0}}\bigg] \end{align} 

\subsection{Deterministic Treatment Regimens} 
This time-varying framework of the treatment variable can also be considered for the covariate, $L$.  From this, a dynamic treatment strategy can also be created, such that each possible realization $\bar{a}_{k}$ is dependent on the treatment and covariate history, $\overline{L}_k$ and $\overline{A}_{k-1}$.  This can be written as a set of functions $\{g_k (\bar{a}_{k-1}, \bar{l}_k )\}$ where $g_k$ is the function making the treatment decision.  

\subsection{Sequentially Randomized Trial} 
The sequentially randomized trial is a specific type of deterministic treatment regimen is , in which a subject's treatment is chosen at each time from an associated density $f (a_k \mid \bar{l}_k, \bar{a}_{k-1})$ for $\bar{a}_k \in \overline{\mathcal{A}}_k$ where $\overline{\mathcal{A}}_k$ is the support of $\overline{A}_k$ in time period $k$.\cite{young2011comparative}  In this type of randomized trial, each $A_k$ for all subjects is chosen as an independent random draw from this type of distribution density.  Sequentially randomized trials guarantee the identifiability assumptions of exchangeability and consistency to be discussed in Section \ref{assumptions}.  This is significantly important because the g-formula and doubly robust estimators are the only methods which can derive causal effect for data with a sequentially randomized trial, particularly important considering how frequently treatments are given in a sequentially randomized fashion in modern medicine.

\section{Confounding} 
%% WHAT IS IT AND WHY DO WE CARE 

The use of the covariate $\overline{L}$ is a measurable proxy for an unmeasured, unknown, and underlying confounder, $U$.  Theoretically, $U$ should directly impact both $L$ and $Y$, but not impact $A$, indicating a backdoor path between $A$ and $Y$ through $U$.  \cite{wright2015international}  The expected way to account for the backdoor path caused by $U$ would be to condition on it, but because it is unknown and therefore unmeasurable, this is not possible.  Therefore, methods must be used to create this same effect using only $L$, which will allow for the study of just the causal effect of $A$ on $Y$.  By eliminating the effect of $U$, there will no bias in the estimate of causal effect.  The methods for doing so will be discussed in Sections \ref{IP Weighting} and \ref{Standardization}.  

In this scenario, $L$ is referred to as a confounder for the effect of $A$, reflective of the fact that the underlying bias is due to the unknown $U$, and $L$ is being used to account for that.  It can be shown that in order to estimate the joint effect of all $A_k$ correctly, simultaneously, and without bias, it is sufficient (but not necessary) to block all backdoor paths from $U$ to any $A_k$ for all $k$.\cite{pearl1995probabilistic}

\section{Identifiability Assumptions} \label{assumptions} 
It is sufficient to show that causal effects are valid and identifiable, meaning they have a single measurement of effect, with the following three assumptions: consistency, positivity, and exchangeability.\cite{cole2009consistency, hernan_robins_2016}   Under these three assumptions, the data closely resembles an ideal randomized trial.  Through this randomness, causation can be inferred, rather than simply association.  Although the methods are directly testing association, these reasonable and founded assumptions allow the tests to measure causation.  
 
\subsection{Consistency} 
Consistency is the idea that an individual's potential outcome and their observed outcome are equal\cite{cole2009consistency, hernan_robins_2016}.  Statistically, this is 
\begin{align} 
\text{If  } A_i = a, \text{     then,    } Y_i^a = Y^{A_i} = Y_i 
\end{align} 
where $Y_i^a$ is individual $i$'s potential outcome and $A_i$ is the observed treatment.  

Consistency can deteriorate under the presence of multiple or varying treatment options, such as if different surgeons perform a procedure or even procedural inconsistencies.  Protection against this is partially in the understanding and reasonable pruning of the data.  This can be addressed with clear and precise questions of interest, and hopefully, detailed data that allows for comprehensive refinement.  

This idea can be expanded to time-varying treatment and covariate variables, as follows 
\begin{align} 
\text{If } \overline{A}_k = \bar{a}^g _k, \text{ then, } \overline{Y}_{k+1} =  \bar{Y}^g_{k+1} \text{ and } \overline{L}_k = \bar{L}^g_k
\end{align}
where $g$ is the function that specifies treatment over time.\footnote{Note that this $g$ is the derivation of the term g-formula.}

\subsection{Exchangeability} \label{exchangeability} 
Exchangeability is the idea that individuals in either group of a randomized experiment would have had the same response given the other treatment. \cite{hernan_robins_2016}  There should be no bias to either group to respond favorably or not to treatment or lack thereof; in short, the results should be equivalent if any subject is moved from one group to the other.  
 
Statistically, this is $P[Y^a = 1 \mid A = 1] = P[Y^a = 1 \mid A = 0] = P[Y^a = 1]$.  This shows that $Y^a$ is independent of $A$, and the treatment has no predictive power of the outcome.  This independence allows for several conclusions.  Firstly, $E[Y^a \mid A = a'] = E[Y^a]$ by the definition of independence.  

Given some indicator of prognosis in the form of $L$, exchangeability is possible for those with similar prognoses, but it becomes problematic across varying prognoses.  For example, exchangeability is attainable when considering obesity, but is more difficult when the confounder has a high mortality rate, such as presence of pneumonia.  Therefore, conditional exchangeability is obtained: $P[Y^a = 1 \mid A = a, L=l] = P[Y^a = 1 \mid A \neq a, L=l]$, i.e. $Y^{a} \Perp A\mid L$. \cite{hernan_robins_2016}  Conditional exchangeability guarantees the ability to measure effects using complete data.  

The use of a randomized trial theoretically creates exchangeability, which is a powerful conclusion that one would like to be able to recreate in observational causal inference studies.   By randomly putting subjects into their groups, there should be no reason that the patients differ between the two groups or will respond to treatment differently.  However, exchangeability can be obtained in an observational study if $P[A_k = 1]$ depends only on $\{\overline{A}_{k-1}, \overline{L}_{k} \}$ and thus, it can be seen that 
\begin{align}
P[A_k \mid \overline{A}_{k-1}, \overline{L}_{k} ] \Perp U
\end{align}
By accounting for $U$ using $L$ in a time-varying treatment method, it can be seen that 
\begin{align} 
Y \Perp A_k \mid  \overline{L}_{k}, \overline{A}_{k-1}
\end{align}
which is statistically referred to as having no unmeasured time-varying confounders.  Although guaranteed for fixed treatments, sequential exchangeability is not guaranteed.\cite{wright2015international}  Approximate exchangeability can be achieved in practice by including as many covariates as is feasibly reasonable, but this is still risky business as there is no known method for computationally measuring or empirically testing sequential exchangeability.  However, the assumption of conditional exchangeability is the same for fixed treatment models and is sufficient for determining causal effect. 

      
\subsection{Positivity} 
Positivity is the condition that a specified conditional probability is well-defined, meaning that for every value of the covariate L, there exist subjects with a specified value of $a$.\cite{hernan2006estimating}  Statistically, this looks like 
\begin{align}
\forall l, \;  \exists P[A=a \mid L=l] > 0 \;\;  \text{, such that } P[L=l] \neq 0
\end{align} 
This can also be expressed for time-varying treatments as follows, 
\begin{align} 
\forall A_k, \; \exists P[A_k = a_k \mid \overline{L}_{k}, \overline{A}_{k-1}] > 0 \;\;   \text{, such that } P[\overline{L}_k = \overline{l}_k, \overline{A}_{k-1} = \overline{a}_{k-1} ] \neq 0
\end{align} 


% Cox (1958) -- independence of observations 
% Rubin 1980 
% no multiple versions of treatment in STUVA 
% check textbook page 5 
%- SUTVA: http://www2.stat.duke.edu/courses/Spring14/sta320.01/Class2.pdf



\section{IP Weighting} \label{IP Weighting} 
Many of the concerns discussed above can be addressed by using the method of IP weighting through simulating a pseudo-population, in which every individual has two data inputs, the expected observed outcomes under treatment and under no treatment.  The method to do this is to consider a confounder of the data, $L$, a value which is known before treatment and often factors into the decision to assign treatment.  For example, a confounder in a study on a cholesterol drug could be whether the patient is obese or has high blood pressure.  By creating the pseudo-population, the treatment and placebo groups share the same underlying covariate characterizations and distributions. 

The pseudo-population can be calculated with the following for each possible combination of $A$ and $L$, 
\begin{align} \label{eq:1}
n\cdot P[Y=y \mid A = a, L= 1] \cdot P[A=a \mid L=l]  \cdot P[L=l] \cdot \frac{1}{P[A = a \mid L = 1]} \end{align}  
where the last term here is the IP weight, $W^{A} = \nicefrac{1}{f(A\mid L)}$.  

%% IS this next statement true?? 
This weight is equivalent to the inverse of the propensity score, which can be defined as the probability of receiving treatment and written as,\cite{imbens2015causal}
\begin{align} 
e(x) = \frac{N_t(x)}{N_c(x) + N_t(x)} = P[A=a \mid L = l] 
\end{align} 
where $x = X_i$ is the population data and $N_t(x)$ and $N_c (x)$ are the number of individuals in the treatment and control groups, respectively.  

The form in expression \ref{eq:1} can be used to solve for the standardized mean as follows, 
\begin{align} 
E[Y^{\,a}] &= \sum_l n \cdot P[Y=y \mid A = a, L= 1] \cdot P[A=a \mid L=l]  \cdot P[L=l] \cdot \frac{1}{P[A = a \mid L = 1]} \\ 
&=  \sum_l n \cdot P[Y=y \mid A = a, L= 1] \cdot P[L=l]\\ 
&= \sum_l E[Y \mid A=a, L= l] P[L=l] 
\end{align} 

This leads to the confounders being either accounted for or eliminated in the pseudo-population.  As a result, the causal effect of $A$ on $Y$ can be effectively estimated using the pseudo-population without any impact from the confounders.  

\subsection{Parametric Estimates} 
The above non-parametric values for $P[A=a\mid L=l]$ are effective for limited dichotomous confounders, but this method has limitations when $L$ is highly dimensional.  To address this, a parametric estimate $\widehat{P}[A=a\mid L=l]$ can be obtained using a logistic regression model for $A$ with all the confounders in $L$ included as covariates.  
% This allows us to estimate IP weights 

\section{Standardization} \label{Standardization} 
Like IP weighting, standardization is a method of calculating the marginal counterfactual risk of $P[Y^a = 1]$.  This method weights the population by  conditioning on the covariates levels in $L$, in order to make the probability of treatment $A$ independent of the covariates.  The weighting can be seen as follows, 
\begin{align} 
P[Y^a = 1] &= \sum_l P[Y^a = 1 \mid L=l] P[L = l] \\ 
&= \sum_l P[Y = 1 \mid L=l] P[L = l]  
\end{align} 
where the equality is because of the conditional exchangeability.  This standardization method can be used to obtain the standardized mean, 
\begin{align} 
E[Y^{\,a}] &= E[Y \mid L = l, A =a ] \cdot P[L=l] 
\end{align} 
Note that this returns the same non-parametric expression for the standardized mean as the method of IP weighting because they are mathematically equivalent.  
