%!TEX root = ../dissertation.tex

\chapter{Background} \label{background}

\newthought{The randomized control trial} introduced in the Introduction seeks to prove causation, allowing for concrete recommendations about the best course of treatment.  The randomization in these trials is what allows the conclusion of causation to be drawn, rather than just correlation.  Randomization intends to eliminate the potential bias caused by factors other than the treatment of interest.  By randomizing subjects into control and treatment groups, neither group should be more likely to respond to treatment.  Because of this, any difference that is seen should be purely due to the treatment and not exogenous factors.  

Randomized control trials are not only costly and slow, but they can also be unethical.  For example, forcing subjects to smoke cigarettes in order to determine if smoking causes lung cancer is not morally appropriate.  As such, the motivation to use observational data to answer such causation questions is strong.  The field of causal inference seeks to accomplish exactly this for both simple observational data and for data from complex dynamically assigned randomized trial.  The field of causal inference began with Neyman in 1990 and Fisher in 1925, who performed agricultural studies to test the efficacy of certain fertilizers on crop yields.\cite{edwards2005ra, fisher1935design, splawa1990application}  Beginning in the 1970s, Rubin advanced and formalized the field, providing more structured frameworks for understanding and computing causal effects.\cite{rosenbaum1984reducing, rubin1974estimating, rubin1978bayesian, rubin1984william} The methods discussed in this thesis and introduced formally in Chapter \ref{Methods}, the g-formula and doubly robust estimators, were introduced by Robins in 1986 and 2005 respectively.\cite{bang2005doubly, robins1986new}

\section{Causal Effects} 
In order to translate these ideas of causal inference into statistical terms, some notation must be introduced.  The random variable $A$ represents the treatment status, where a value of $1$ indicates treated and a value of $0$ indicates untreated.  A fixed $A$, which has constant treatment over time, is written as $A_i$ for an individual $i \in \{0,1,2,\dots n\}$ with $n$ the total number of individuals.\footnote{Non-fixed $A$ representations are common and discussed in greater detail in Section \ref{Time-varying}.} The random variable $Y$ is the outcome variable, often with a value of $0$ indicating survival and a value of $1$ indicating death for studies on terminal diseases. Other possible example of a binary $Y$ could be outcomes such as reaching a specific percent change in weight loss or a decrease in cholesterol levels.   These interpretations of $A$ and $Y$ correspond to the above understanding of causation studies, but for various causal inference studies, the form of $Y$ and $A$ in particular can change depending on the question of interest.  For example, $Y$ can be a continuous variable, such as the weight difference of an individual in a weight loss trial or the change in HDL levels in a cholesterol study.  

To study the causal effect of $A$ on $Y$, the desired value to estimate is the difference in $Y$ under the varying conditions of $A$.  Notationally, this is the difference between $Y^{\, a=1}$ \footnote{Note that lowercase letters signify possible values of the random variable, in comparison to uppercase letters which represent actual observed values}, the outcome that would be observed under treatment, and $Y^{\, a=0}$, the outcome that would be observed under no treatment.  This idea of a ``would be observed" value is referred to as the counterfactual, and this is in comparison to the actual observed outcome of $Y$ or $Y^A$.  

A causal effect can be seen on an individual level if the inequality $\; Y_i^{\, a=1} \neq Y_i^{\, a=0}$ is significant.  By considering how each individual's responses to varying treatments differ, causation (or lack thereof) can easily be determined using paired differences of the form 
\begin{align} \; Y_i^{\, a=1} - Y_i^{\, a=0}\end{align} 
These differences would be tested against the null hypothesis of zero difference in outcome for varying treatments.  

However, certain difficulties arise using this method.  In many studies, it is impossible to have scenarios of both treatment and no treatment for the same individual, particularly if a potential outcome is death.  Typically, individuals either have $\; Y_i^{\, a=1}$ or  $Y_i^{\, a=0}$, but not both, making it impossible to calculate the paired differences.  Therefore, a controlled double blinded experiment is often performed, where each individual is randomly assigned treatment or placebo.  In these studies, the statistic of interest is the average causal effect in the population, 
\begin{align}  \mathbb{E}[Y^{\; a=1}] - \mathbb{E}[Y^{\; a=0}] \end{align} 
Mathematically, this is equivalent to 
\begin{align}  \mathbb{E}[Y^{\; a=1} - Y^{\; a=0}] \end{align}  
because the average of differences is equal to the difference of averages.\cite{hernan_robins_2016} Note, that this is not the same as calculating the mean of paired differences as if each individual had received both treatments at different times to calculate individual causal effects.  Rather, the difference in the means of the placebo and treatment groups is being calculated to estimate average causal effect across the population.  

\subsection{Time Varying Data} \label{Time-varying}
Not all treatment regimens consist of constant treatment over a set period of time.  Therefore, a more complicated time-varying treatment can be considered.  This would be written for a single individual as $\overline{A}_K = \{A_0, A_1, \cdots, A_K \}$, with time point $k \in \{ 0, 1, \dots K\}$, given $K$ as the maximum time value.  The overline on $\overline{A}_k$ indicates the history of values up to and including time point $k$, and the notation $\overline{A}$ represents the full history.  As an example of this, a patient with continuous treatment throughout the whole study would have data $\overline{A} = \{A_0 = 1, A_1 = 1, \dots, A_K = 1 \}  = \{1,1,\dots 1 \}$, which can also be written as $\overline{A} = \overline{1}$.  In this scenario, the average causal effect is instead defined as 
\begin{align} \mathbb{E}\bigg[Y^{\bar{a} = \bar{1}}\bigg] -  \mathbb{E}\bigg[Y^{\bar{a} = \bar{0}}\bigg] \end{align} 

\subsection{Deterministic Treatment Regimens} 
This time-varying framework of the treatment variable can also be considered for the covariate, $L$.\footnote{A more complete description of the covariate $L$ can be seen in Section \ref{confounding}.}  From this, a dynamic treatment strategy can also be created, such that each possible realization $\bar{a}_{k}$ is dependent on the treatment and covariate history, $\overline{L}_k$ and $\overline{A}_{k-1}$.  This can be written as a set of functions $\{g_k (\bar{a}_{k-1}, \bar{l}_k )\}$ where $g_k$ is the function making the treatment decision.  

\subsection{Sequentially Randomized Trial} 
The sequentially randomized trial is a specific type of treatment regimen, in which a subject's treatment is chosen at each time from an associated density $f (a_k \mid \bar{l}_k, \bar{a}_{k-1})$ for $\bar{a}_k \in \overline{\mathcal{A}}_k$ where $\overline{\mathcal{A}}_k$ is the support of $\overline{A}_k$ in time period $k$.\cite{young2011comparative}  In this type of randomized trial, each $A_k$ for all subjects is chosen as an independent random draw from this type of distribution density.  Sequentially randomized trials guarantee the identifiability assumptions of exchangeability and consistency to be discussed in Section \ref{assumptions}.  This is significantly important because the g-formula and doubly robust estimators are the only current methods which can derive causal effect for data with a sequentially randomized trial.  Although these models are not that widespread yet, they are considered more informative and with proper mechanisms to understand their results, they will likely become much more widely used.

\section{Confounding} \label{confounding}
In observational data, many measured covariates exist, which we call $L$; these could include subject height, weight, age, smoking status, medical history, family history, past treatments, and blood levels.  Each covariate $L$ is something that could possibly be correlated with the outcome $Y$. The full covariate history $\overline{L}$ is a measurable proxy for an unmeasured, unknown, and underlying confounder, $U$, such as a genetic predisposition to specific diseases.  This is to prevent $U$ from creating any bias in measures of causal effect; thus, any measured covariate that can be obtained should likely be used with the goal of covering $U$ as much as possible. It has been shown that if an unmeasured confounder is not sufficiently accounted for, there will be bias in the final estimates.\cite{vanderweele2011unmeasured}

Theoretically, $U$ should directly impact both $L$ and $Y$, but not directly impact $A$ because $A$ is only directly dependent on its own history and the history of $L$.\footnote{See the schematic drawing in Figure \ref{schematic} for a visualization of these connections.}  This indicates that there exists a backdoor path between $A$ and $Y$ through $U$.\cite{wright2015international}  The expected way to account for the backdoor path caused by $U$ would be to condition on it, but because it is unknown and therefore unmeasurable, this is not possible.  Therefore, methods must be used to create this same effect using only $L$, which will allow for the study of just the causal effect of $A$ on $Y$.  If the effect of $U$ on $Y$ is fully accounted for, then there should be no bias in the estimate of causal effect.  Methods for eliminating the effect of $U$ on $Y$ will be discussed in Sections \ref{IP Weighting} and \ref{Standardization}.  

In this scenario, $L$ is referred to as a confounder for the effect of $A$, reflective of the fact that the underlying bias is due to the unknown $U$, and $L$ is being used to account for that.  It can be shown that in order to estimate the joint effect of all $A_k$ correctly, simultaneously, and without bias, it is sufficient (but not necessary) to block all backdoor paths from $U$ to any $A_k$ for all $k$.\cite{pearl1995probabilistic}

\section{Identifiability Assumptions} \label{assumptions} 
It is sufficient to show that causal effects are valid and identifiable, meaning they have a single measurement of effect, with the following three assumptions: consistency, positivity, and exchangeability.\cite{cole2009consistency, hernan_robins_2016}   Under these three assumptions, the data closely resembles an ideal randomized trial.  Through this randomness, causation can be inferred, rather than simply association.  Although the methods are directly testing association, these reasonable and founded assumptions allow the tests to measure causation.\footnote{These assumptions are very similar to Rubin's exclusion restrictions, including the Stable Unit Treatment Value Assumption (SUTVA).\cite{rubin1980randomization}  Rubin defines these exclusion restrictions as those which ``rely on external, substantive information to rule out the existence of a causal effect of a particular treatment relative to an alternative.''\cite{imbens2015causal} SUTVA is the assumption no individual's treatment impacts the outcome of any other subjects.}

It is important also to consider the assumptions of no measurement error and no model misspecification.  Measurement error is a common assumption that is hard to quantify, but could lead to significantly biased results.  Model specification is in the hands of the researchers and if done incorrectly, will do an insufficient job at adjusting for confounding, resulting in similar biases as created by measurement errors.  
 
\subsection{Consistency} 
Consistency is the idea that an individual's potential outcome and their actual observed outcome are equal\cite{cole2009consistency, hernan_robins_2016}.  Specifically, this is 
\begin{align} 
\text{If  } A_i = a, \text{     then,    } Y_i^a = Y^{A_i} = Y_i 
\end{align} 
where $Y_i^a$ is individual $i$'s potential counterfactual outcome and $A_i$ is the observed treatment.  

Consistency can deteriorate under the unintentional presence of varying treatment options.  For example, this can be violated if different surgeons perform a procedure, different procedures are performed, or if different drugs or doses are administered to treat the same condition.  There are many reasons doctors could use a non standard treatment causing these inconsistencies to appear, such as a family history, complications with a procedure or a drug, or another condition.  Protection against violating this assumption is partially in the understanding and reasonable pruning of the data.  Furthermore, it can also be addressed with clear and precise questions of interest, and hopefully, detailed data that allows for comprehensive refinement.  However, this is a very difficult assumption to ensure without meticulous data cleaning and research about the data acquisition.  By including as many relevant covariates, $L$, the goal is to eliminate as much inconsistency as possible, but this assumption is impossible to ever fully guarantee.  

This idea can be expanded to time-varying treatment and covariate variables, as follows 
\begin{align} 
\text{If } \overline{A}_k = \bar{a}^g _k, \text{ then, } \overline{Y}_{k+1} =  \bar{Y}^g_{k+1} \text{ and } \overline{L}_k = \bar{L}^g_k
\end{align}
where $g$ is the function that specifies treatment over time.\footnote{Note that this $g$ is the origin of the term g-formula.}

\subsection{Exchangeability} \label{exchangeability} 
Exchangeability is the idea that individuals in either group of a randomized experiment would have had the same response given the other treatment. \cite{hernan_robins_2016}  There should be no bias for either group to respond favorably or not to treatment or lack thereof; in short, the results should be equivalent if any subject is moved from one group to the other.  Under exchangeability, the counterfactual mean should produce the same results regardless of which individuals actually got treatment and which did not.  Statistically, this is $P[Y^a = 1 \mid A = 1] = P[Y^a = 1 \mid A = 0] = P[Y^a = 1]$ for both $a=0$ and $a=1$.  This means that $Y^a$ is independent of $A$ and that the actual treatment $A$ does not predict the outcome under the counterfactual, $Y^a$.  

Given some indicator of prognosis in the form of $L$, exchangeability is possible for those with similar prognoses, but it becomes problematic across varying prognoses.  For example, two individuals with similar medical history would be expected to have the same counterfactual outcome under exchangeability, but comparing the counterfactual outcome of two subjects with pneumonia, one elderly and one a child, would not induce the exchangeability.  Therefore, it is important to condition on such confounders, allowing for conditional exchangeability: $P[Y^a = 1 \mid A = a, L=l] = P[Y^a = 1 \mid A \neq a, L=l]$, i.e. $Y^{a} \Perp A\mid L$. \cite{hernan_robins_2016}  Conditional exchangeability guarantees the ability to measure effects using complete data because the effect measured is just that of treatment and not the underlying confounders, since they should be equivalent in both groups by exchangeability.  

The use of a randomized trial theoretically creates exchangeability, which is a powerful conclusion that one would like to be able to recreate in observational causal inference studies.   By randomly putting subjects into their groups, there should be no reason that the patients differ between the two groups or will respond to treatment differently.  However, exchangeability can be obtained in an observational study if $P[A_k = 1]$ depends only on $\{\overline{A}_{k-1}, \overline{L}_{k} \}$. Thus, the important assumption of exchangeability in regards to observational data is that  $ P[A_k \mid \overline{A}_{k-1}, \overline{L}_{k} ]$ is independent of $U$.   A consequence of accounting for $U$ using $L$ in this way is that $Y$ is independent of $A_k \mid  \overline{L}_{k}, \overline{A}_{k-1}$.  This is statistically referred to as having no unmeasured time-varying confounders.  


Although guaranteed for fixed treatments, sequential exchangeability is not guaranteed.\cite{wright2015international}  Approximate exchangeability can be achieved in practice by including as many covariates as is feasibly reasonable, but this is still risky business as there is no known method for computationally measuring or empirically testing sequential exchangeability.  For example, a study could include all the confounders regarding medical history, physicians on the case, and symptoms, but could neglect to include a confounder such as who owns a particular piece of equipment and is getting paid for its use, meaning they would be more motivated to prescribe such treatment.  Such a confounder would be unexpected and hard to quantify, potentially introducing violations to the necessary assumptions.  By including a large number of other covariates, the hope is that it will be approximately accounted for, but this is one of many examples where it is impossible to determine if it will truly be guaranteed conditional exchangeability.  The assumption of conditional exchangeability is the same for fixed treatment models, and importantly, it is sufficient for determining causal effect. 

      
\subsection{Positivity} 
Positivity is the idea that every possible treatment, covariate condition is represented in the data set.  This is the condition that all specified conditional probabilities are well-defined, meaning that for every value of the covariate $L$, there exist subjects with a specified value of $a$.\cite{hernan2006estimating} 
\begin{align}
\text{If }P[L=l] \neq 0 \; \; \forall l, \text{ then } \;  \exists P[A=a \mid L=l] > 0 \;\;  
\end{align} 
This can also be expressed for time-varying treatments as follows, 
\begin{align} 
\text{If } P[\overline{L}_k = \overline{l}_k, \overline{A}_{k-1} = \overline{a}_{k-1} ] \neq 0 \; \; \forall A_k, \text{ then }  \; \exists P[A_k = a_k \mid \overline{L}_{k}, \overline{A}_{k-1}] > 0 
\end{align} 


% Cox (1958) -- independence of observations 
% Rubin 1980 
% no multiple versions of treatment in STUVA 
% check textbook page 5 
%- SUTVA: http://www2.stat.duke.edu/courses/Spring14/sta320.01/Class2.pdf


\section{Inverse Probability (IP) Weighting} \label{IP Weighting} 
Many of the concerns discussed above can be addressed by using the method of Inverse Probability (IP) weighting through simulating a pseudo-population, in which every individual has two data inputs, the expected observed outcomes under treatment and under no treatment.  The method to do this is to consider a confounder of the data, $L$, a value which is known before treatment and often factors into the decision to assign treatment.  For example, a confounder in a study on a cholesterol drug could be whether the patient is obese or has high blood pressure.  By creating the pseudo-population, the treatment and placebo groups share the same underlying covariate characterizations and distributions. 

The pseudo-population can be calculated with the following for each possible combination of $A$ and $L$, 
\begin{align} \label{eq:1}
n\cdot P[Y=y \mid A = a, L= l] \cdot P[A=a \mid L=l]  \cdot P[L=l] \cdot \frac{1}{P[A = a \mid L = l]} \end{align}  
where the last term here is the IP weight, $W^{A} = \nicefrac{1}{f(A\mid L)}$.  

%% IS this next statement true?? 
This weight is equivalent to the inverse of the propensity score, which can be defined as the probability of receiving treatment and written as,\cite{imbens2015causal}
\begin{align} 
e(x) = \frac{N_t(x)}{N_c(x) + N_t(x)} = P[A=a \mid L = l] 
\end{align} 
where $x = X_i$ is the true population data and $N_t(x)$ and $N_c (x)$ are the number of individuals in the treatment and control groups, respectively.  

The form in expression \ref{eq:1} can be used to solve for the standardized mean as follows, 
\begin{align} 
E[Y^{\,a}] &= \sum_l \frac{1}{n} \cdot P[Y=y \mid A = a, L= 1] \cdot P[A=a \mid L=l]  \cdot P[L=l] \cdot \frac{1}{P[A = a \mid L = 1]} \\ 
&=  \sum_l \frac{1}{n} \cdot P[Y=y \mid A = a, L= 1] \cdot P[L=l]\\ 
&= \sum_l E[Y \mid A=a, L= l] P[L=l] 
\end{align} 

This leads to the confounders being either accounted for or eliminated in the pseudo-population.  As a result, the causal effect of $A$ on $Y$ can be effectively estimated using the pseudo-population without any impact from the confounders.  

\subsection{Parametric Estimates} 
The above non-parametric values for $P[A=a\mid L=l]$ are effective for limited dichotomous confounders, but this method has limitations when $L$ is highly dimensional.  To address this, a parametric estimate $\widehat{P}[A=a\mid L=l]$ can be obtained using a logistic regression model for $A$ with all the confounders in $L$ included as covariates.  
% This allows us to estimate IP weights 

\section{Standardization} \label{Standardization} 
Like IP weighting, standardization is a method of calculating the marginal counterfactual risk of $P[Y^a = 1]$.  This method weights the population by  conditioning on the covariates levels in $L$, in order to make the probability of treatment $A$ independent of the covariates.  The weighting can be seen as follows, 
\begin{align} 
P[Y^a = 1] &= \sum_l P[Y^a = 1 \mid L=l] P[L = l] \\ 
&= \sum_l P[Y = 1 \mid A=1, L=l] P[L = l]  
\end{align} 
where the equality holds because of the conditional exchangeability.  This standardization method can be used to obtain an estimate of the standardized mean, 
\begin{align} 
E[Y^{\,a}] &= \sum_{l} E[Y \mid L = l, A =a ] \cdot P[L=l] 
\end{align} 
Note that this returns the same non-parametric expression for the standardized mean as the method of IP weighting because they are mathematically equivalent.  
