%!TEX root = ../dissertation.tex

\chapter{Methods} \label{Methods}


\newthought{Two formal methods for estimating} causal effect are considered in this study: g-formula estimation and doubly robust estimation.  These two methods are developments of standardization and IP weighting as discussed in the previous section.  They were implemented and studied using simulated data according to the following methods.  

\section{Data Creation} \label{data}
Specifically for the purposes of this study, a data generating algorithm was engineered to provide consistent and easily accessible data for many simulations.  The data generated was time-varying and sequentially randomized according to the following schematic.  

\begin{figure}[h!]
  \begin{displaymath}
    \xymatrix{
    	& & & & & & \mathbf{Y} \\
        \mathbf{L_0} \ar[r] & \mathbf{A_0} \ar[r]  & \mathbf{ L_1} \ar[r]  & \mathbf{A_1} \ar@{.>}[r] & \mathbf{L_K} \ar[r] & \mathbf{A_K} &  \\ 
        & & & & & \ar[lllllu] \ar[lllu]  \ar[lu] \mathbb{U}  \ar[ruu]  & 
        }
\end{displaymath}
  \centering{\caption[Schematic of data generating algorithm]{Diagram of conditional dependencies in data generating process.} \label{schematic}}
 \end{figure}


The algorithm to generate datasets is as follows for each individual, of which 1,000 were simulated in this study.  The time variable $k$ took on values $\{0, \dots K=11\}$.  
\begin{enumerate} 
\item Determine the coefficients $\vec{\mathbf{\alpha}}$ and $\vec{\mathbf{\beta}} $, which are the parameters that define the data generating process in the following models, 
\begin{align} 
Logit[L_{k,i}] &= \alpha_0 + \alpha_1 \cdot L_{k-1,i} + \alpha_2 \cdot L_{k-2,i} + \alpha_3 A_{k-1,i} + \alpha_4 A_{k-2,i} + \alpha_5 U_i \label{eq:10}\\ 
Logit[A_{k,i}] &= \beta_0 + \beta_1 L_{k,i} + \beta_2 L_{k-1,i} + \beta_3 A_{k-1,i} + \beta_4 A_{k-2,i} \label{eq:11}
\end{align} 

These parameters are generated outside the data generating process to provide consistency.  In this study, the values were as follows, 
\begin{align*}   
\begin{tabular}{L|L}
\; \; \; \;\; \; \; \;\; \; \; \; \vec{\mathbf{\alpha}} &\; \; \; \;\; \; \; \; \; \; \; \;  \vec{\mathbf{\beta}} \\ 
\hline 
\alpha_0 = 0.58986656 & \beta_0 = 0.17868818\\ 
\alpha_1 = 0.95344212 & \beta_1 = 0.89069712 \\ 
\alpha_2 = -0.89822429 & \beta_2 =   0.89037635 \\ 
\alpha_3 =  -0.95566697 & \beta_3 = 0.20497534 \\ 
\alpha_4 = 0.67520365 & \beta_4 =  0.10442911 \\ 
 \alpha_5 = 2.46365403 & 
\end{tabular} 
\end{align*}  
These coefficients were created by pulling each from $\vec{\mathbf{\alpha}},\vec{\mathbf{\beta}} \sim Uniform(-1.0, 1.0)$ in order to get a variety of positive and negative parameters.  The one change made was that $\alpha_5$ had 1.5 added to the randomly generated value to ensure that the underlying, unmeasured covariate, $U$, had significant impact for testing purposes.  

\item Create the underlying confounder, $U_i$ from $U_i \sim Unif(0.1, 1)$. 
\item Using the logistic expressions \ref{eq:10} and \ref{eq:11}, probabilities for each $L_{k,i}$ and $A_{k,i}$ can be obtained where $k$ is the time and $i$ is the individual, conditional on $\overline{L}_{k-1,i}$ and $\overline{A}_{k-1,i}$.  These probabilities are then used to obtain values for $L_{k,i}$ and $A_{k,i}$ using a binomial distribution with the respective probabilities.  

Note that for lower values of time when history is limited, the above expressions are slightly modified as follows, 
\begin{align} 
Logit[L_{0,i}] &= \alpha_0 + \alpha_5 U_i\\ 
Logit[A_{0,i}] &= \beta_0 + \beta_1 L_{k,i} \\
Logit[L_{1,i}] &= \alpha_0 + \alpha_1 \cdot L_{k-1,i} + \alpha_3 A_{k-1,i}  + \alpha_5 U_i\\ 
Logit[A_{1,i}] &= \beta_0 + \beta_1 L_{k,i} + \beta_2 L_{k-1,i} + \beta_3 A_{k-1,i} 
\end{align} 
\item Obtain a final $Y_i$ value for each individual where $Y_i \sim Binom(p=expit(0.5+U_i))$.  \label{datastep4} 
\end{enumerate} 

Note that the unmeasured, underlying covariate, $U$ directly influences each $L_k$ and $Y$, but does not directly influence $A_k$.  This is an important assumption so that $Y$ can be independent of $A_k \mid \overline{A}_{k-1}, \overline{L}_k$ such that $\overline{L}_k$ is adjusting for $U$.  This is an assumption that is difficult to guarantee in true observational data.  

Furthermore, the final outcome $Y$ value is independent of $A$ and therefore, the treatment has no impact on the outcome under the null hypothesis.  This was done to ensure that the causal treatment effect would be zero and bias could be easily measured throughout the study.  In reality, it is unlikely that one would be testing truly under the null, so a few tests were performed under an alternative hypothesis of treatment effect in Section \ref{alternative}.  In order to induce such a treatment effect, step \ref{datastep4} was change to $Y_i \sim Binom(p = expit(-1+U_i + A_K + \mathbb{E}(A)))$ where $\mathbb{E}(A) = \frac{1}{K} \sum_{i=1}^K A_i $ is the computational mean of the treatments throughout history.  

\section{Parametric G-formula} 
Similar to IP weighting, parametric estimates can be obtained for standardized estimates.  An efficient method for doing this is the generalization of standardization to time-varying treatments and confounders, coined the g-formula method by Robins in 1986.\cite{hernan_robins_2016, robins1986new, wright2015international}  The method can be used for fixed and time-varying treatments in longitudinal studies, and it seeks to estimate the average causal effect of treatment, which can be estimated as 
\begin{align} 
\mathbb{E}[Y^{\bar{a} = \bar{1}}] - \mathbb{E}[Y^{\bar{a} = \bar{0}}] 
\end{align} 
where the respective $\bar{a} = \bar{1}$ and $\bar{a} = \bar{0}$ signify constant treatment and no treatment over the entire time period.  

%% SINCE I CHANGED TO LOGISTIC, DOES THIS BECOME PRODUCT OF THE PROBABILITIES
The g-formula seeks to calculate each standardized mean using the following, 
\begin{align} \label{eq:3} 
\mathbb{E}[Y^{\bar{a}= \bar{1}}] &= \sum_{l_i} Logit \big[Y \mid  \overline{L}_{t}, \overline{A}_{t} \big]\cdot \prod_{k=0}^t P[L_k = l_k \mid \overline{L}_{k-1}, \overline{A}_{k-1}]
\end{align}
where $\overline{L}_k = \{L_{0} = l_0, L_{1} = l_1,  \cdots, \; L_{k} = l_k\}$ and $\overline{A}_k = \{a_{0} = 1, a_{1} = 1,  \cdots, \; a_{k} = 1\}$ are the history of the treatment and covariate variables up to and including time $k$.  The equivalent formula can be derived for $ \mathbb{E}[Y^{\bar{a} = \bar{0}}]$.   

%In expression \ref{eq:3}, the summation term is %% finish this sentence
%% come back here to discuss more about why you would use this method, how it was developed, details from Jamie's original paper, etc

One of the key reasons for using the g-formula method is that it is able to account for time-varying confounders which have feedback to each other.  This is equivalent to each $L_k$ being dependent on $A_{k-1}$.\cite{robins1986new}  In these scenarios, traditional methods for adjusting for the confounder, such as stratification, regression, and matching may introduce bias. However, the g-formula method (as well as IP weighting) is able estimate the joint effect of all treatment values $\{A_0, A_1 \dots A_K \}$ simultaneously and without bias, which these other methods are unable to do.\cite{fitzmaurice2008longitudinal, wright2015international}  

The g-formula method has been shown to have a smaller variance than IP weighting methods, but this comes with added parametric modeling assumptions.\cite{young2011comparative} The smaller variance is due to the fact that the g-formula uses maximum likelihood estimates, in comparison to the semi-parametric estimator used in IP weighting. Furthermore, IP weighting does fault and become quite unstable under violations (or close violations) of the positivity assumption, due to division by a potentially near zero probability $P[A_k=a_k \mid \overline{L}_k, \overline{A}_{k-1}]$.  

These improvements are, however, under the assumption of exchangeability, and the fact that the g-formula relies more heavily on parametric assumptions, which can lead to bias.  The presence of bias is dependent on the accuracy of the models for $Y$, $A_k$ and $L_k$ for all $k$.  IP weighting methods are also dependent on the accuracy of their models, just different models such as for $A_k$ conditional on $\overline{L}_k, \overline{A}_{k-1}$.  

%% NEED TO LOOK INTO THE G-NULL PARADOX 
%% "theorem which implies that it can be essentially impossible to specify correct parametric models under the causal null hypothesis (i.e. the true risk difference under any two choices of x is exactly zero).  As a consequence, the method will reject the causal null even when true in sufficiently large samples." \cite{young2011comparative}

\subsection{Protocol} 
The method is performed in several steps, as follows 
\begin{enumerate}  

\item \underline{Create outcome models}: Create models for the outcome variable $Y$ and the covariates, $L_k$ at each time using the original dataset.  The model for $Y$ is regressed on the treatment variable $A$ and the confounders, $L$. 

In this case, the following models were chosen for $Y \mid  \overline{A}_t, \overline{L}_t$ and $L_k \mid \overline{L}_{k-1}, \overline{A}_{k-1}$, 
\begin{align} 
Logit \big[Y \mid \overline{A}_t, \overline{L}_t \big] &= \theta_{0} + \theta_1 A_{t} + \cdots + \theta_j A_0 + \theta_{j+1} L_t + \cdots + \theta_{j+k} L_0 \label{eq:4} \\ 
Logit[L_k \mid \overline{L}_{k-1}, \overline{A}_{k-1}] &= \gamma_0 + \gamma_1 L_{k-1} + \gamma_2 L_{k-2} + \gamma_3 L_{k-3}  + \gamma_4 A_{k-1} + \gamma_5 A_{k-2} + \gamma_6 A_{k-3} \label{eq:5} 
\end{align} 
A time lag of only three historical values was deemed sufficient for the model of $L_k$ because each of the $L_k$ are dependent on previous time history so three historical values should be sufficient to capture that. 
%% PUT SOME REASONS IN HERE DISCUSSING HOW THESE MODELS WERE CHOSEN... SHOULD I BE USING QUADRATIC TERMS OR INTERACTION TERMS??!?!?!?!


Note that for initial time points where there was insufficient history for the full model, smaller models were created as follows 
\begin{align} 
Logit[L_1 \mid L_0, A_0]  &= \gamma'_0 + \gamma'_1 L_0 +  \gamma'_2 A_0 \label{eq:6} \\
Logit[L_2 \mid L_0, L_1, A_0, A_1] &= \gamma''_0 + \gamma''_1 L_{k-1} + \gamma''_2 L_{k-2}   + \gamma''_3 A_{k-1} + \gamma''_4 A_{k-2} \label{eq:7}
\end{align}


\item \underline{Predict using Monte Carlo}: Using the model created in step 2, predict the outcome $Y$ for the two new datasets created in step 1, conditioned on the given $A$ and $L$ values.  

Using expressions \ref{eq:4} through \ref{eq:7}, a Monte Carlo simulation must be performed to gain prediction values.  This is because it is impractical to calculate expression \ref{eq:5} directly for a continuous $L$.  This process is done as follows for time $k = \{ 0, \dots, K \}$, and individuals $i = \{ 1, \dots, n \}$ keeping the test treatment regimen of interest $\bar{a}$ in mind through the process.  
\begin{enumerate} 
\item Select the $L_0$ value from a random individual from $i \in  \{ 1, \dots, n \}$. \label{step1}  
\item Obtain a probability of $L_1$ using this $L_0$ and $a_0$ in expression \ref{eq:6} and then obtain a sample value of $L_1$ by pulling from a binomial distribution.  
\item Obtain a probability of $L_2$ using the $L_0, L_1, a_0$, and $a_1$ in expression \ref{eq:7} and then obtain a sample value of $L_2$ by pulling from a binomial distribution.  
\item Continue the same process until time $K$ using expression \ref{eq:5} to get a full history $\overline{L}_K$ and all the probabilities $P[L_k = l_k \mid  \overline{L}_{k-1}, \overline{A}_{k-1}]$.  
\item Using expression \ref{eq:4}, $\bar{a}$ and the above solved for $\overline{L}_K$, calculate $P \big[Y \mid \overline{A}_K, \overline{L}_K \big]$.  
\item Take the product of all the probabilities $P[L_k = l_k \mid  \overline{L}_{k-1}, \overline{A}_{k-1}]$ for $k = 0, \dots, K$ and $\mathbb{P} \big[Y \mid \overline{A}_K, \overline{L}_K \big]$ to get a final estimate.  \label{laststep}
\item Repeat steps (\ref{step1}) through (\ref{laststep}) for as many simulations as desired.  In this study, 10,000 individuals were simulated.    
\item Take the mean of all simulation values to obtain $\mathbb{E}[Y^{\bar{a}}]$.  
\item Repeat all above steps for the opposing treatment regimen of interest $\bar{a}'$ and take the difference $\mathbb{E}[Y^{\bar{a}}] - \mathbb{E}[Y^{\bar{a}'}]$ to get the average causal treatment effect.  
\end{enumerate}
\end{enumerate} 

To do this process using non-parametric estimates, the Monte Carlo simulation is unnecessary.  Instead, the methodology would be to create two new simulated datasets, the first having all individuals under no treatment ($A=0$) and the second having all individuals treated ($A=1$).  Each of these new datasets has the same size as the original and the same ``individuals'', meaning the same covariate $L$ distribution, just changed values for $A$.  For these two datasets, delete the outcome values for $Y$ to treat it as a missing data point.  Then, the outcome models would be calculated as above.  The prediction step, however, would differ, instead using the models directly to get predicted values for each individual in the extra two datasets, rather than by a Monte Carlo simulation.  Finally, the standardized means could be obtained by creating a weighted average for $E[Y^{a=0}]$ from the first new dataset and one for $E[Y^{a=1}]$ from the second new dataset.  

\section{Doubly Robust Estimation} 
The method of doubly robust estimation, as proposed by Bang and Robins,\cite{bang2005doubly} combines the two previously discussed methods of IP weighting and standardization.  IP weighting estimates $P[A=a \mid L =l]$, while standardization estimates $P[Y \mid A = a, L=l]$ and $P[\overline{L}_k =l_k \mid \overline{L}_{k-1}, \overline{A}_{k-1}]$.  Therefore, these two techniques are expected to provide different answers, unless there are no models used to create estimates as would be the case if all estimates were non-parametric.  \cite{hernan_robins_2016}   

The method of doubly robust estimation does not make use of the observed data treatment density, as the methods of IP weighting and standardization do. This allows for some use of missing data points, rather than just having to drop these data points.  This also prevents a lack of skewing due to overrepresented populations that could result from missing data.  It will be shown in Chapter \ref{Results} that the estimators derived are consistent if either the model for treatment given the past (as in IP weighting) is correctly specified or the models for the outcome and covariates given the past (as needed to implement the parametric g-formula) are correctly specified, without knowing which is correct.  It is only when both models are misspecified that the method breaks.  This is the derivation of the term doubly robust.  Furthermore, however, evidence will be presented showing that this method is actually more than doubly robust.  Previously, it has been shown that either model being correctly specified for all time will prevent the introduction of bias.  However, this thesis demonstrates that the model does not have to be correctly specified for the entire time. Section \ref{doublerobust} presents the results that the two models can be correctly specified in specific order combinations without introducing bias.  

\subsection{Protocol} 
The method can be performed recursively using the following steps, 

\begin{enumerate}
\item Build a model for the treatment $A_k$ with data pooled for all time $m \in \{1, \dots, K \}$ and all individuals $i \in \{1, \dots n\}$ and obtain the MLE $\hat{\mathbf{\alpha}}$ of $\mathbf{\alpha}$ using logistic regression. 
\begin{align} 
logit\{P[A_{m,i} = 1 \mid \overline{l}_{m,i}, \overline{a}_{m-1,i}; \mathbf{\alpha}]\} &= w_m [\overline{l}_{m,i}, \overline{a}_{m-1,i}; \mathbf{\alpha}]
\end{align} 

This model can be rewritten as the following,  
\begin{align} 
f(A_m \mid \overline{L}_m, \overline{A}_{m-1}; \hat{\mathbf{\alpha}}) = \alpha_{0} + \alpha_{1} \cdot L_{m} + \alpha_{2} \cdot A_{m-1} + \alpha_{3} \cdot L_{m-1} + \alpha_{4} \cdot L_{m-2} + \alpha_{5} \cdot A_{m-2} 
\end{align} 

\item Set $\hat{T}_{K+1} = Y$.  

\item Recurse for $m = K+1, \dots, 2$ 
\begin{enumerate}
\item \label{modcreate} Use iteratively re-weighted least squares (IRLS) and a specified parametric regression model to get
\begin{align}
h_{m-1}(\overline{L}_{m-1}, \overline{A}_{m-1}; \mathbf{\beta}_{m-1}, \phi_{m-1}) = \Psi \{s_{m-1}(\overline{L}_{m-1}, \overline{A}_{m-1}; \mathbf{\beta}_{m-1}) + \phi_{m-1} \overline{\pi}_{m-1}^{-1} (\hat{\mathbf{\alpha}}) \}
\end{align}
which gives the conditional expectation of 
\begin{align}
\mathbb{E} \bigg[\hat{T}_m \mid \overline{L}_{m-1}, \overline{A}_{m-1} \bigg]
\end{align} 
The known function $s_{m}$ is specified on a case by case basis, and in this case was chosen to be as follows for the unknown parameter $\mathbf{\beta}$.  
\begin{align}
s_{m}(\overline{L}_{m}, \overline{A}_{m};\mathbf{\beta}_{m}) = \beta_0 + \beta_1 L_{m} +\beta_2 A_{m} + \beta_3 L_{m-1} +\beta_4 A_{m-1}+ \beta_5 L_{m-2} +\beta_6 A_{m-2}
\end{align}

Furthermore, the function $\overline{\pi}_m(\hat{\mathbf{\alpha}})$ is the propensity score model and is specified as follows
\begin{align} 
\overline{\pi}_m(\hat{\mathbf{\alpha}}) &= \prod_{j=1}^m f(A_m \mid \overline{L}_m, \overline{A}_{m-1}; \hat{\mathbf{\alpha}}) \\
&= \zeta_0 + \zeta_1 L_m + \zeta_2 L_{m-1} + \zeta_3 A_{m-1} + \zeta_4 A_{m-2}
\end{align}
The given $\Psi$ is the canonical link function of the chosen GLM.  The desired method to do this is using a GLM with an underlying distribution (or family) of a Gaussian normal and a logit link.  However, Python does not have the capacity to do it this way, so alternatives had to be tested and considered, including logistic regression, basic linear regression with an expit applied after step \ref{stepc} as well as using a logistic regression and taking the predicted probability to pull 1000 samples from a binomial distribution for each individual and regressing off that new data in the next step. However, through much testing, it was concluded that the best means to do this was using a GLM with an underlying binomial distribution and a logit link.  

\item Let $ \hat{h}_{m-1}(\overline{L}_{m-1}, \overline{A}_{m-1}; \hat{\mathbf{\beta}}_{m-1}, \hat{\phi}_{m-1})$ be the predicted model derived in step \ref{modcreate}.  This implies that $(\hat{\mathbf{\beta}}'_{m-1}, \hat{\phi}_{m-1}')$ is a solution of 
\begin{align}
\mathbf{0} = \tilde{\mathbb{E}} \left[ \left[\hat{T}_{m} - \Psi \{ s_{m-1}(\overline{L}_{m-1}, \overline{A}_{m-1}; \hat{\mathbf{\beta}}_{m-1}) + \hat{\phi}_{m-1} \overline{\pi}^{-1}_{m-1}(\hat{\mathbf{\alpha}}) \} \right] \left( \frac{\partial s (\overline{L}_{m-1}; \mathbf{\beta}_{m-1})}{\partial \mathbf{\beta}'_{m-1}, \overline{\pi}^{-1}_{m-1}(\hat{\mathbf{\alpha}})} \right) \right]
\end{align}
where $\tilde{\mathbb{E}}(X) = \frac{1}{n} \sum_{i=1}^n X_i$ is the computational average.  

\item \label{stepc} Set 
\begin{align} 
\hat{T}_{m-1}^{a_{m-1}, \dots, a_K} &= \hat{h}_{m-1}(\overline{L}_{m-1}, \overline{A}_{m-2}, a_{m-1}) \\
&=  \Psi \{s_{m-1}(\overline{L}_{m-1}, \overline{A}_{m-2}, a_{m-1}; \mathbf{\beta}_{m-1}) + \phi_{m-1} \overline{\pi}_{m-2}^{-1} (\hat{\mathbf{\alpha}}) f(a_{m-1} \mid \overline{L}_{m-1}, \overline{A}_{m-2}; \hat{\mathbf{\alpha}}) \}
\end{align} 
where $a_{m-1}$ is our treatment value of interest, the lowercase letter indicating a test value rather than an observed.  
\end{enumerate}

\item To calculate the final $\mathbb{E}[Y^{\bar{a}}]$, solve 
\begin{align} 
\mathbb{E}[Y^{\bar{a}}] = \tilde{\mathbb{E}}(\hat{T}_1) = \tilde{\mathbb{E}}(\hat{T}_1^{\bar{a}})
\end{align}

\item Repeat all above steps for the opposing treatment regimen of interest $\bar{a}'$ and take difference $\mathbb{E}[Y^{\bar{a}}] - \mathbb{E}[Y^{\bar{a}'}]$ to get the average causal treatment effect.  
\end{enumerate}

\section{Variance Estimate} \label{VarianceBootStrap}
In order to compute the variance of the estimates obtained using the above two methods, a bootstrapping simulation was conducted.  This was done by repeating the above processes 1,000 times and collecting all of the resulting estimates.  The variance of these estimates was then obtained.  

\subsection{Protocol}
\begin{enumerate} 
\item Determine the number of simulations to be performed.  In this case, 1,000 simulations were performed.  \label{stepa} 
\item Perform the following steps as many times as decided in step \ref{stepa} 
\begin{enumerate}
\item Create a dataset using the data generating algorithm described in Section \ref{data}.  
\item Estimate the average causal treatment effect using the g-formula.  
\item Estimate the average causal treatment effect using the doubly robust method.  
\end{enumerate} 
\item Calculate the mean of estimates for each of the two methods 
\item Calculate the variance and standard error of each mean of estimates.  
\end{enumerate} 
%% can we confirm in here whether I am getting a variance of the estimate (i.e. the method) or a variance of the mean? 

Note that it is also possible to directly compute the variance of a doubly robust estimator, but this was beyond the scope of this project.  The ability to calculate variance without bootstrapping is highly efficient and proves another advantage of the doubly robust estimator.  


